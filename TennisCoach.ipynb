{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Pose Estimation using AlphaPose**"
      ],
      "metadata": {
        "id": "tP0b07J1TS8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone repository:"
      ],
      "metadata": {
        "id": "HpTQ6gf2jCAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/christinathr2000/TennisCoach.git"
      ],
      "metadata": {
        "id": "8I9AbNXuQYqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99728eee-45c4-4bfb-a523-9ac23442e914"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TennisCoach'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Counting objects: 100% (291/291), done.\u001b[K\n",
            "remote: Compressing objects: 100% (241/241), done.\u001b[K\n",
            "remote: Total 291 (delta 40), reused 243 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (291/291), 52.31 MiB | 26.13 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install requirements:"
      ],
      "metadata": {
        "id": "vI_8-FThjJ04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/TennisCoach/AlphaPose"
      ],
      "metadata": {
        "id": "LIKa3O9CidHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c5700a-70cc-45ca-95f9-c47d4456ae32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TennisCoach/AlphaPose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "aWGEI6Zbie3e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks if GPU is available:"
      ],
      "metadata": {
        "id": "5lcp67wLjLTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "5YhfiWJuQYcP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e04bc7f-31b2-41b0-d491-baa363539c40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloads used SPPE and YOLO detector and stores in correct folder:** \\\\\n",
        "Lately, an access denied error occurs when using gdown, even though this should not happen.\n",
        "If you get that message, download the file at the shown link and add to\n",
        "/TennisCoach/AlphaPose/models/sppe folder.\\\\\n",
        "Below, you'll find a code snippet that served as our solution for the gdown error. We stored all the data on our drive and imported it from there. Feel free to follow the same approach."
      ],
      "metadata": {
        "id": "3Ah027tgjQ5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd models/sppe"
      ],
      "metadata": {
        "id": "Co5aFt5NikJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022f58cc-d416-4318-ba61-a5ba4c9a7215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TennisCoach/AlphaPose/models/sppe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1OPORTWB2cwd5YTVBX-NE8fsauZJWsrtW\n",
        "# Lately, a access denied error occurs - even though this should not happen.\n",
        "# If you get that message, download the file at the shown link and add to /TennisCoach/AlphaPose/models/sppe folder"
      ],
      "metadata": {
        "id": "6P_Qf48OiurH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2f9783-305d-4cc2-c6c5-6a1203b1da3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1OPORTWB2cwd5YTVBX-NE8fsauZJWsrtW \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../yolo"
      ],
      "metadata": {
        "id": "o66oIkTQivGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da185047-37c1-40ba-c89a-03a7ade01eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TennisCoach/AlphaPose/models/yolo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC\n",
        "# Lately, a access denied error occurs - even though this should not happen.\n",
        "# If you get that message, download the file at the shown link and add to /TennisCoach/AlphaPose/models/yolo folder\n",
        "# or use cells underneath to get data from Google Drive"
      ],
      "metadata": {
        "id": "P6RTZWwPiyFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4609ec77-5632-48ce-d3b9-66ae7d0f7d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fanzuguFQb6z",
        "outputId": "43f89ecd-4121-435b-ae62-afd47b9774e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "sppe_path = '/content/drive/My Drive/PersonalTennisCoach/duc_se.pth'\n",
        "new_sppe_path = '/content/TennisCoach/AlphaPose/models/sppe/'\n",
        "shutil.copy(sppe_path, new_sppe_path)\n",
        "\n",
        "yolo_path = '/content/drive/My Drive/PersonalTennisCoach/yolov3-spp.weights'\n",
        "new_yolo_path = '/content/TennisCoach/AlphaPose/models/yolo/'\n",
        "shutil.copy(yolo_path, new_yolo_path)"
      ],
      "metadata": {
        "id": "Bb2IVmKUQcr8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbf25af8-5c2e-461b-cce1-7ad08ac9cc18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TennisCoach/AlphaPose/models/yolo/yolov3-spp.weights'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Video Number:** \\\\\n",
        "We provide some test videos on our github and can be found in the input folder.\n",
        "Feel free to set video_nr to your selected number."
      ],
      "metadata": {
        "id": "3LYG5t2CBqK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_nr = 11\n",
        "long_nr = str('{:03d}'.format(video_nr))\n",
        "video_path = 'input/video' + long_nr + '.mp4'"
      ],
      "metadata": {
        "id": "3cMeMegrBt6k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Preprocessing\n",
        "We check framerate, resolution and video format. \\\\\n",
        "The histogram equalization part is commented out because AlphaPose also did not use it during training.\n"
      ],
      "metadata": {
        "id": "0daMjDCTyNoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/TennisCoach/AlphaPose"
      ],
      "metadata": {
        "id": "3oc4uUS3iz4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35968b3-93eb-42f8-f37b-a9b3bb35323b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TennisCoach/AlphaPose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import magic\n",
        "\n",
        "def video_preprocessing(video_path):\n",
        "    #Check if file is mp4\n",
        "    file_type = magic.Magic().from_file(video_path)\n",
        "    if 'MP4' not in file_type.upper():\n",
        "      print(\"Error: The file format is not MP4.\")\n",
        "      return False\n",
        "\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not video.isOpened():\n",
        "        print(\"Error: Video file was not opened successfully\")\n",
        "        return False\n",
        "\n",
        "    # Get the frames per second (fps) of the video and check if minimum of 20 fps is met\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    if fps < 13:\n",
        "      print(\"Error: Frame rate is too low. A minimum rate of 20 fps is required.\")\n",
        "      video.release()\n",
        "      return False\n",
        "\n",
        "    #Check resolution of video\n",
        "    width = int(video.get(3))\n",
        "    height = int(video.get(4))\n",
        "    resolution = (width, height)\n",
        "\n",
        "    if (width < 300 or height < 300):\n",
        "      print(\"Error: Resolution is too small. A minimum resolution of 300x300 is needed.\")\n",
        "      video.release()\n",
        "      return False\n",
        "\n",
        "    #Perform histogram equalization to increase visibility ----------------------------\n",
        "    #https://www.geeksforgeeks.org/saving-a-video-using-opencv/\n",
        "    #We do not perform Histogram equalization anymore because AlphaPose also\n",
        "    #did not use it during training\n",
        "    # out_video_path = 'input/video' + long_nr + '_equal.mp4'\n",
        "    # out_file = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    #                         fps, (width, height))\n",
        "    #Inspiration from: https://docs.opencv.org/3.4/dd/d43/tutorial_py_video_display.html\n",
        "    # while True:\n",
        "    #   ret, frame = video.read()\n",
        "    #   if not ret:\n",
        "    #     break\n",
        "    #   grey = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
        "    #   grey[:,:,0] = cv2.equalizeHist(grey[:,:,0])\n",
        "    #   rgb_equal = cv2.cvtColor(grey, cv2.COLOR_YCrCb2BGR)\n",
        "      #out_file.write(rgb_equal)\n",
        "\n",
        "    # Release video capture and writer objects\n",
        "    # video.release()\n",
        "    # out_file.release()\n",
        "    #-----------------------------------------------------------------------------------\n",
        "\n",
        "    # Closes all the frames\n",
        "    cv2.destroyAllWindows()\n",
        "    return True\n",
        "\n",
        "rvalue = video_preprocessing(video_path)\n",
        "if not rvalue:\n",
        "  print(\"Something went wrong, check the above error message before continuing.\")\n",
        "else:\n",
        "  print(\"Everything looks good!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY6hwVPOyNDk",
        "outputId": "5b0d0159-4237-430d-82cf-fbdcd0fb9bb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pose Estimation is performed with AlphaPose"
      ],
      "metadata": {
        "id": "h80WAP7XjavY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "command = \"python video_demo.py --video {} --outdir output --save_video --sp --vis_fast\".format(video_path)\n",
        "!{command}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqWE-obXBCi5",
        "outputId": "9a96b8a4-c356-476a-a2c8-41547d379f61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading YOLO model..\n",
            "Loading pose model from ./models/sppe/duc_se.pth\n",
            "100% 62/62 [00:06<00:00, 10.00it/s]\n",
            "===========================> Finish Model Running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get Phase Estimation of each frame"
      ],
      "metadata": {
        "id": "37dwROY100-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../PhaseDetection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkhLs3ZmT1xj",
        "outputId": "6e4a84ee-4664-41ed-97ea-971f7415ad63"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TennisCoach/PhaseDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import VideoPhaseEstimator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1OKSteiTBNp",
        "outputId": "6fab3c69-be42-4afc-96c8-32624040429c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The console stream is logged into /root/sg_logs/console.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2024-01-25 10:40:43] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
            "[2024-01-25 10:40:44] WARNING - __init__.py - Failed to import pytorch_quantization\n",
            "[2024-01-25 10:40:44] INFO - utils.py - NumExpr defaulting to 2 threads.\n",
            "[2024-01-25 10:40:54] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
            "[2024-01-25 10:40:54] WARNING - export.py - Failed to import pytorch_quantization\n",
            "[2024-01-25 10:40:54] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n",
            "[2024-01-25 10:40:54] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: boto3 required but not found\u001b[0m\n",
            "[2024-01-25 10:40:54] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: deprecated required but not found\u001b[0m\n",
            "[2024-01-25 10:40:54] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: coverage required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: sphinx-rtd-theme required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: torchmetrics required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: hydra-core required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: omegaconf required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnxruntime required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnx required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: einops required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: treelib required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: stringcase required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: rapidfuzz required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: json-tricks required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnx-simplifier required but not found\u001b[0m\n",
            "[2024-01-25 10:40:55] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: data-gradients required but not found\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/TennisCoach/AlphaPose/\" + video_path\n",
        "video_phase_estimator = VideoPhaseEstimator.VideoPhaseEstimator()\n",
        "frame_phase = video_phase_estimator.estimate_phases(path)\n",
        "\n",
        "#store frame number for each phase in list for keypoint analysis\n",
        "start_nr = [idx for idx, v in enumerate(frame_phase) if v == 'start']\n",
        "trophy_nr = [idx for idx, v in enumerate(frame_phase) if v == 'loading']\n",
        "acc_nr = [idx for idx, v in enumerate(frame_phase) if v == 'acceleration']\n",
        "contact_nr = [idx for idx, v in enumerate(frame_phase) if v == 'contact']\n",
        "finish_nr = [idx for idx, v in enumerate(frame_phase) if v == 'finish']\n",
        "\n",
        "#To have time intervals for final annotated video\n",
        "s_start = min(start_nr) + (max(start_nr)-min(start_nr))/2 if min(start_nr) + (max(start_nr)-min(start_nr))/2 >= 0 else 0\n",
        "s_trophy = max(start_nr)\n",
        "s_acc = max(trophy_nr)\n",
        "s_contact = max(acc_nr)\n",
        "s_finish = max(contact_nr)\n",
        "e_finish = max(finish_nr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCygY6pFSNnn",
        "outputId": "a02ddbbf-9182-4123-82e0-939c5b39d283"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2024-01-25 10:40:57] INFO - checkpoint_utils.py - License Notification: YOLO-NAS pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n",
            "Downloading: \"https://sghub.deci.ai/models/yolo_nas_l_coco.pth\" to /root/.cache/torch/hub/checkpoints/yolo_nas_l_coco.pth\n",
            "100%|██████████| 256M/256M [00:01<00:00, 180MB/s]\n",
            "[2024-01-25 10:41:00] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_l\n",
            "[2024-01-25 10:41:01] INFO - checkpoint_utils.py - License Notification: YOLO-NAS-POSE pre-trained weights are subjected to the specific license terms and conditions detailed in \n",
            "https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS-POSE.md\n",
            "By downloading the pre-trained weight files you agree to comply with these terms.\n",
            "Downloading: \"https://sghub.deci.ai/models/yolo_nas_pose_l_coco_pose.pth\" to /root/.cache/torch/hub/checkpoints/yolo_nas_pose_l_coco_pose.pth\n",
            "100%|██████████| 304M/304M [00:06<00:00, 52.3MB/s]\n",
            "[2024-01-25 10:41:09] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_pose_l\n",
            "[2024-01-25 10:41:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n",
            "[2024-01-25 10:41:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find mistakes in each extracted phase:"
      ],
      "metadata": {
        "id": "w3eS0aJJ3HC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/TennisCoach/AlphaPose"
      ],
      "metadata": {
        "id": "y74rIpPuzBTd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#Have an array with error codes we can then use in the visualization to extract\n",
        "#bounding boxes around needed areas\n",
        "mistakes = []\n",
        "# Mistakes in Start Phase:\n",
        "#  0 - Left foot not in front of right foot\n",
        "#  1 - Feet are too close together\n",
        "#  2 - Feet are too far apart\n",
        "#  3 - Let your left arm dangle\n",
        "#  4 - Let your right arm dangle\n",
        "# Mistakes in Trophy Phase:\n",
        "#  5 - Right arm should be in a 90° angle - under 80°\n",
        "#  6 - Right arm should be in a 90° angle - over 100°\n",
        "#  7 - Left arm should be already above your head\n",
        "#  8 - Left arm should be more stretched\n",
        "#  9 - Bow your shoulder, hips and feet more\n",
        "# 10 - Bow your left and right body part equally\n",
        "# Mistakes in Acceleration Phase:\n",
        "# 11 - bow your shoulder, hips and feet more\n",
        "# 12 - bow your left and right body part equally\n",
        "# Mistakes in Contact Point Phase:\n",
        "# 13 - straight arm - angle between shoulder - elbow - wrist ~ 180°\n",
        "# 14 - stretch left leg more\n",
        "# 15 - stretch right leg more\n",
        "# Mistakes in Finish Phase:\n",
        "# 16 - bend knees more to protect knees (compared to contact point phase)\n",
        "\n",
        "def checkPosition():\n",
        "  with open('output/alphapose-results.json') as json_result:\n",
        "    data = json.load(json_result)  #list of dictionaries\n",
        "\n",
        "  #1. remove additional persons --------------------------------------------\n",
        "  #indices are added to list 'idx_to_remove'\n",
        "  idx_to_remove = []\n",
        "  for idx, dic in enumerate(data):\n",
        "    if idx > 0 and dic['image_id'] == data[idx-1]['image_id']:\n",
        "      for idx2 in range(idx-1, -1, -1):\n",
        "        if dic['image_id'] != data[idx2]['image_id']:\n",
        "          break\n",
        "        if idx2 in idx_to_remove:\n",
        "          continue\n",
        "        height_1 = max(data[idx2]['keypoints'][46] - data[idx2]['keypoints'][1], data[idx2]['keypoints'][49] - data[idx2]['keypoints'][1])\n",
        "        height_2 = max(dic['keypoints'][46] - dic['keypoints'][1], dic['keypoints'][49] - dic['keypoints'][1])\n",
        "        if(height_2 > height_1):\n",
        "          idx_to_remove.append(idx2)\n",
        "        else:\n",
        "          idx_to_remove.append(idx)\n",
        "        break\n",
        "\n",
        "  idx_to_remove = list(dict.fromkeys(idx_to_remove)) # remove duplicates\n",
        "  idx_to_remove.sort(reverse=True) #sorts indices in descending order - so we don't have an indexing error\n",
        "  #removes additional persons\n",
        "  for idx in idx_to_remove:\n",
        "    data.pop(idx)\n",
        "\n",
        "  with open('output/alphapose-results.json', \"w\") as json_file:\n",
        "    json.dump(data, json_file)\n",
        "\n",
        "  start_dicts, trophy_dicts, acc_dicts, contact_dicts, finish_dicts = [], [], [], [], []\n",
        "  for idx, dic in enumerate(data):\n",
        "    if idx in start_nr:\n",
        "      start_dicts.append(dic)\n",
        "    elif idx in trophy_nr:\n",
        "      trophy_dicts.append(dic)\n",
        "    elif idx in acc_nr:\n",
        "      acc_dicts.append(dic)\n",
        "    elif idx in contact_nr:\n",
        "      contact_dicts.append(dic)\n",
        "    elif idx in finish_nr:\n",
        "      finish_dicts.append(dic)\n",
        "\n",
        "  #-------------------------------------------------------------------------\n",
        "  #2. check start phase ----------------------------------------------------\n",
        "  # We concentrate on the analysis of videos taken from the right side and right-handed people\n",
        "  mistakes.extend([0,1,2,3,4])\n",
        "  for start_dict in start_dicts:\n",
        "    keypoints_start = start_dict['keypoints']\n",
        "    #keypoint order from https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md\n",
        "    nose = (keypoints_start[0], keypoints_start[1])\n",
        "    left_eye = (keypoints_start[3], keypoints_start[4])\n",
        "    right_eye = (keypoints_start[6], keypoints_start[7])\n",
        "    left_ear = (keypoints_start[9], keypoints_start[10])\n",
        "    right_ear = (keypoints_start[12], keypoints_start[13])\n",
        "    left_shoulder = (keypoints_start[15], keypoints_start[16])\n",
        "    right_shoulder = (keypoints_start[18], keypoints_start[19])\n",
        "    left_elbow = (keypoints_start[21], keypoints_start[22])\n",
        "    right_elbow = (keypoints_start[24], keypoints_start[25])\n",
        "    left_wrist = (keypoints_start[27], keypoints_start[28])\n",
        "    right_wrist = (keypoints_start[30], keypoints_start[31])\n",
        "    left_hip = (keypoints_start[33], keypoints_start[34])\n",
        "    right_hip = (keypoints_start[36], keypoints_start[37])\n",
        "    left_knee = (keypoints_start[39], keypoints_start[40])\n",
        "    right_knee = (keypoints_start[42], keypoints_start[43])\n",
        "    left_ankle = (keypoints_start[45], keypoints_start[46])\n",
        "    right_ankle = (keypoints_start[48], keypoints_start[49])\n",
        "\n",
        "    #2.1. Left foot in front of right foot\n",
        "    if(left_ankle[0] > right_ankle[0]):\n",
        "      if 0 in mistakes:\n",
        "        mistakes.remove(0)\n",
        "\n",
        "    #2.2. Feet are approximately shoulder-width apart or slightly wider\n",
        "    #Transform x-y coordinates to cm with resolution\n",
        "    distance_feet_x = left_ankle[0] - right_ankle[0]\n",
        "    distance_feet_y = left_ankle[1] - right_ankle[1]\n",
        "    distance_feet = math.sqrt(distance_feet_x**2 + distance_feet_y**2)\n",
        "    distance_shoulder_x = left_shoulder[0] - right_shoulder[0]\n",
        "    distance_shoulder_y = left_shoulder[1] - right_shoulder[1]\n",
        "    distance_shoulder = math.sqrt(distance_shoulder_x**2 + distance_shoulder_y**2)\n",
        "\n",
        "    if distance_feet > distance_shoulder*1.1:\n",
        "      if 1 in mistakes:\n",
        "        mistakes.remove(1)\n",
        "    if distance_feet < distance_shoulder*1.5:\n",
        "      if 2 in mistakes:\n",
        "       mistakes.remove(2)\n",
        "\n",
        "    #Hands are pointing down - wrists under elbows - elbows under shoulder\n",
        "    if not(left_wrist[1] < left_elbow[1] and left_elbow[1] < left_shoulder[1] and left_wrist[1] < left_hip[1]):\n",
        "      if 3 in mistakes:\n",
        "        mistakes.remove(3)\n",
        "    if not(right_wrist[1] < right_elbow[1] and right_elbow[1] < right_shoulder[1] and right_wrist[1] < right_hip[1]):\n",
        "      if 4 in mistakes:\n",
        "        mistakes.remove(4)\n",
        "\n",
        "  #-------------------------------------------------------------------------\n",
        "  #3. check trophy phase ---------------------------------------------------\n",
        "  mistakes.extend([5,6,7,8,9,10])\n",
        "  for trophy_dict in trophy_dicts:\n",
        "    keypoints_trophy = trophy_dict['keypoints']\n",
        "    #keypoint order from https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md\n",
        "    nose = (keypoints_trophy[0], keypoints_trophy[1])\n",
        "    left_eye = (keypoints_trophy[3], keypoints_trophy[4])\n",
        "    right_eye = (keypoints_trophy[6], keypoints_trophy[7])\n",
        "    left_ear = (keypoints_trophy[9], keypoints_trophy[10])\n",
        "    right_ear = (keypoints_trophy[12], keypoints_trophy[13])\n",
        "    left_shoulder = (keypoints_trophy[15], keypoints_trophy[16])\n",
        "    right_shoulder = (keypoints_trophy[18], keypoints_trophy[19])\n",
        "    left_elbow = (keypoints_trophy[21], keypoints_trophy[22])\n",
        "    right_elbow = (keypoints_trophy[24], keypoints_trophy[25])\n",
        "    left_wrist = (keypoints_trophy[27], keypoints_trophy[28])\n",
        "    right_wrist = (keypoints_trophy[30], keypoints_trophy[31])\n",
        "    left_hip = (keypoints_trophy[33], keypoints_trophy[34])\n",
        "    right_hip = (keypoints_trophy[36], keypoints_trophy[37])\n",
        "    left_knee = (keypoints_trophy[39], keypoints_trophy[40])\n",
        "    right_knee = (keypoints_trophy[42], keypoints_trophy[43])\n",
        "    left_ankle = (keypoints_trophy[45], keypoints_trophy[46])\n",
        "    right_ankle = (keypoints_trophy[48], keypoints_trophy[49])\n",
        "\n",
        "    # 2.1. 90° angle of right arm\n",
        "    elbow_wrist = np.array([right_wrist[0] - right_elbow[0], right_wrist[1] - right_elbow[1]])\n",
        "    elbow_shoulder = np.array([right_shoulder[0] - right_elbow[0], right_shoulder[1] - right_elbow[1]])\n",
        "    # Formula from https://www.cuemath.com/geometry/angle-between-vectors/\n",
        "    rad_angle = math.acos(np.dot(elbow_wrist,elbow_shoulder) / (np.linalg.norm(elbow_wrist) * np.linalg.norm(elbow_shoulder)))\n",
        "    deg_angle = rad_angle * ( 180.0 / math.pi)\n",
        "    if deg_angle < 80:\n",
        "      if 5 in mistakes:\n",
        "        mistakes.remove(5)\n",
        "    if deg_angle < 100:\n",
        "      if 6 in mistakes:\n",
        "        mistakes.remove(6)\n",
        "\n",
        "    # 2.2. left hand up\n",
        "    # 2.2.1. left wrist over nose\n",
        "    if left_wrist[1] < nose[1]:\n",
        "      if 7 in mistakes:\n",
        "        mistakes.remove(7)\n",
        "\n",
        "    # 2.2.2. angle of left arm between 90 and 180°\n",
        "    l_elbow_wrist = np.array([left_wrist[0] - left_elbow[0], left_wrist[1] - left_elbow[1]])\n",
        "    l_elbow_shoulder = np.array([left_shoulder[0] - left_elbow[0], left_shoulder[1] - left_elbow[1]])\n",
        "    rad_angle = math.acos(np.dot(l_elbow_wrist,l_elbow_shoulder) / (np.linalg.norm(l_elbow_wrist) * np.linalg.norm(l_elbow_shoulder)))\n",
        "    deg_angle = rad_angle * ( 180.0 / math.pi)\n",
        "    if deg_angle >= 90: #I guess stetching an arm over 180° is not possible!\n",
        "      if 8 in mistakes:\n",
        "        mistakes.remove(8)\n",
        "\n",
        "    # 2.3. bow of shoulder, hips and feet\n",
        "    # check if angle is smaller than 180°\n",
        "    r_hip_shoulder = np.array([right_shoulder[0] - right_hip[0], right_shoulder[1] - right_hip[1]])\n",
        "    r_hip_ankle = np.array([right_ankle[0] - right_hip[0], right_ankle[1] - right_hip[1]])\n",
        "    # Formula from https://www.cuemath.com/geometry/angle-between-vectors/\n",
        "    r_rad_angle = math.acos(np.dot(r_hip_shoulder,r_hip_ankle) / (np.linalg.norm(r_hip_shoulder) * np.linalg.norm(r_hip_ankle)))\n",
        "    r_deg_angle = r_rad_angle * (180.0 / math.pi)\n",
        "    if not (r_deg_angle > 180 and right_shoulder[0] > right_hip[0] and right_ankle[0] > right_hip[0]):\n",
        "      if 9 in mistakes:\n",
        "        mistakes.remove(9)\n",
        "\n",
        "    l_hip_shoulder = np.array([left_shoulder[0] - left_hip[0], left_shoulder[1] - left_hip[1]])\n",
        "    l_hip_ankle = np.array([left_ankle[0] - left_hip[0], left_ankle[1] - left_hip[1]])\n",
        "    l_rad_angle = math.acos(np.dot(l_hip_shoulder,l_hip_ankle) / (np.linalg.norm(l_hip_shoulder) * np.linalg.norm(l_hip_ankle)))\n",
        "    l_deg_angle = l_rad_angle * (180.0 / math.pi)\n",
        "    lower_bound = r_deg_angle * 0.9\n",
        "    upper_bound = r_deg_angle * 1.1\n",
        "    if l_deg_angle > lower_bound and l_deg_angle < upper_bound:\n",
        "      if 10 in mistakes:\n",
        "        mistakes.remove(10)\n",
        "\n",
        "  #-------------------------------------------------------------------------\n",
        "  #3. check acceleration phase ---------------------------------------------\n",
        "  mistakes.extend([11,12])\n",
        "  for acc_dict in acc_dicts:\n",
        "    keypoints_acc = acc_dict['keypoints']\n",
        "    #keypoint order from https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md\n",
        "    nose = (keypoints_acc[0], keypoints_acc[1])\n",
        "    left_eye = (keypoints_acc[3], keypoints_acc[4])\n",
        "    right_eye = (keypoints_acc[6], keypoints_acc[7])\n",
        "    left_ear = (keypoints_acc[9], keypoints_acc[10])\n",
        "    right_ear = (keypoints_acc[12], keypoints_acc[13])\n",
        "    left_shoulder = (keypoints_acc[15], keypoints_acc[16])\n",
        "    right_shoulder = (keypoints_acc[18], keypoints_acc[19])\n",
        "    left_elbow = (keypoints_acc[21], keypoints_acc[22])\n",
        "    right_elbow = (keypoints_acc[24], keypoints_acc[25])\n",
        "    left_wrist = (keypoints_acc[27], keypoints_acc[28])\n",
        "    right_wrist = (keypoints_acc[30], keypoints_acc[31])\n",
        "    left_hip = (keypoints_acc[33], keypoints_acc[34])\n",
        "    right_hip = (keypoints_acc[36], keypoints_acc[37])\n",
        "    left_knee = (keypoints_acc[39], keypoints_acc[40])\n",
        "    right_knee = (keypoints_acc[42], keypoints_acc[43])\n",
        "    left_ankle = (keypoints_acc[45], keypoints_acc[46])\n",
        "    right_ankle = (keypoints_acc[48], keypoints_acc[49])\n",
        "\n",
        "    # 3.2. bow of shoulder, hips and feet\n",
        "    # check if angle is smaller than 180°\n",
        "    r_hip_shoulder = np.array([right_shoulder[0] - right_hip[0], right_shoulder[1] - right_hip[1]])\n",
        "    r_hip_ankle = np.array([right_ankle[0] - right_hip[0], right_ankle[1] - right_hip[1]])\n",
        "    r_rad_angle = math.acos(np.dot(r_hip_shoulder,r_hip_ankle) / (np.linalg.norm(r_hip_shoulder) * np.linalg.norm(r_hip_ankle)))\n",
        "    r_deg_angle = r_rad_angle * (180.0 / math.pi)\n",
        "    if r_deg_angle < 180 and right_shoulder[0] < right_hip[0] and right_ankle[0] < right_hip[0]:\n",
        "      if 11 in mistakes:\n",
        "        mistakes.remove(11)\n",
        "\n",
        "    l_hip_shoulder = np.array([left_shoulder[0] - left_hip[0], left_shoulder[1] - left_hip[1]])\n",
        "    l_hip_ankle = np.array([left_ankle[0] - left_hip[0], left_ankle[1] - left_hip[1]])\n",
        "    l_rad_angle = math.acos(np.dot(l_hip_shoulder,l_hip_ankle) / (np.linalg.norm(l_hip_shoulder) * np.linalg.norm(l_hip_ankle)))\n",
        "    l_deg_angle = l_rad_angle * (180.0 / math.pi)\n",
        "    lower_bound = r_deg_angle * 0.9\n",
        "    upper_bound = r_deg_angle * 1.1\n",
        "    if l_deg_angle > lower_bound and l_deg_angle < upper_bound:\n",
        "      if 12 in mistakes:\n",
        "        mistakes.remove(12)\n",
        "\n",
        "  #-------------------------------------------------------------------------\n",
        "  # 4. check contact point phase -------------------------------------------\n",
        "  mistakes.extend([13,14,15])\n",
        "  for contact_dict in contact_dicts:\n",
        "    keypoints_contact = contact_dict['keypoints']\n",
        "    #keypoint order from https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md\n",
        "    nose = (keypoints_contact[0], keypoints_contact[1])\n",
        "    left_eye = (keypoints_contact[3], keypoints_contact[4])\n",
        "    right_eye = (keypoints_contact[6], keypoints_contact[7])\n",
        "    left_ear = (keypoints_contact[9], keypoints_contact[10])\n",
        "    right_ear = (keypoints_contact[12], keypoints_contact[13])\n",
        "    left_shoulder = (keypoints_contact[15], keypoints_contact[16])\n",
        "    right_shoulder = (keypoints_contact[18], keypoints_contact[19])\n",
        "    left_elbow = (keypoints_contact[21], keypoints_contact[22])\n",
        "    right_elbow = (keypoints_contact[24], keypoints_contact[25])\n",
        "    left_wrist = (keypoints_contact[27], keypoints_contact[28])\n",
        "    right_wrist = (keypoints_contact[30], keypoints_contact[31])\n",
        "    left_hip = (keypoints_contact[33], keypoints_contact[34])\n",
        "    right_hip = (keypoints_contact[36], keypoints_contact[37])\n",
        "    left_knee = (keypoints_contact[39], keypoints_contact[40])\n",
        "    right_knee = (keypoints_contact[42], keypoints_contact[43])\n",
        "    left_ankle = (keypoints_contact[45], keypoints_contact[46])\n",
        "    right_ankle = (keypoints_contact[48], keypoints_contact[49])\n",
        "\n",
        "    # 4.1. straight arm - angle between shoulder - elbow - wrist = 180°\n",
        "    elbow_wrist = np.array([right_wrist[0] - right_elbow[0], right_wrist[1] - right_elbow[1]])\n",
        "    elbow_shoulder = np.array([right_shoulder[0] - right_elbow[0], right_shoulder[1] - right_elbow[1]])\n",
        "    rad_angle = math.acos(np.dot(elbow_wrist,elbow_shoulder) / (np.linalg.norm(elbow_wrist) * np.linalg.norm(elbow_shoulder)))\n",
        "    deg_angle = rad_angle * (180.0 / math.pi)\n",
        "    if deg_angle > 170:\n",
        "      if 13 in mistakes:\n",
        "        mistakes.remove(13)\n",
        "\n",
        "    # 4.2. hitting point at maximum height (not sure, how to do that)  (TODO - ask Sebastian)\n",
        "    #check if legs are also stretched\n",
        "    #left leg\n",
        "    left_knee_ankle = np.array([left_knee[0] - left_ankle[0], left_knee[1] - left_ankle[1]])\n",
        "    left_knee_hip = np.array([left_knee[0] - left_hip[0], left_knee[1] - left_hip[1]])\n",
        "    rad_angle = math.acos(np.dot(left_knee_ankle,left_knee_hip) / (np.linalg.norm(left_knee_ankle) * np.linalg.norm(left_knee_hip)))\n",
        "    deg_angle = rad_angle * (180.0 / math.pi)\n",
        "    if deg_angle > 165:\n",
        "      if 14 in mistakes:\n",
        "        mistakes.remove(14)\n",
        "\n",
        "    #right leg\n",
        "    right_knee_ankle = np.array([right_knee[0] - right_ankle[0], right_knee[1] - right_ankle[1]])\n",
        "    right_knee_hip = np.array([right_knee[0] - right_hip[0], right_knee[1] - right_hip[1]])\n",
        "    rad_angle = math.acos(np.dot(right_knee_ankle,right_knee_hip) / (np.linalg.norm(right_knee_ankle) * np.linalg.norm(right_knee_hip)))\n",
        "    deg_angle = rad_angle * (180.0 / math.pi)\n",
        "    if deg_angle > 165:\n",
        "      if 15 in mistakes:\n",
        "        mistakes.remove(15)\n",
        "\n",
        "  #-------------------------------------------------------------------------\n",
        "  # 5. check finish phase --------------------------------------------------\n",
        "\n",
        "  #angles from contact point phase to compare with angle from finish phase\n",
        "  prev_l_knee_ankle = np.array([left_ankle[0] - left_knee[0], left_ankle[1] - left_knee[1]])\n",
        "  prev_l_knee_hip = np.array([left_knee[0] - left_hip[0], left_knee[1] - left_hip[1]])\n",
        "  prev_l_rad_angle = math.acos(np.dot(prev_l_knee_ankle,prev_l_knee_hip) / (np.linalg.norm(prev_l_knee_ankle) * np.linalg.norm(prev_l_knee_hip)))\n",
        "  prev_l_deg_angle = prev_l_rad_angle * (180.0 / math.pi)\n",
        "\n",
        "  prev_r_knee_ankle = np.array([right_ankle[0] - right_knee[0], right_ankle[1] - right_knee[1]])\n",
        "  prev_r_knee_hip = np.array([right_knee[0] - right_hip[0], right_knee[1] - right_hip[1]])\n",
        "  prev_r_rad_angle = math.acos(np.dot(prev_r_knee_ankle,prev_r_knee_hip) / (np.linalg.norm(prev_r_knee_ankle) * np.linalg.norm(prev_r_knee_hip)))\n",
        "  prev_r_deg_angle = prev_r_rad_angle * (180.0 / math.pi)\n",
        "\n",
        "  mistakes.append(16)\n",
        "  for finish_dict in finish_dicts:\n",
        "    keypoints_finish = finish_dict['keypoints']\n",
        "    #keypoint order from https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/output.md\n",
        "    nose = (keypoints_finish[0], keypoints_finish[1])\n",
        "    left_eye = (keypoints_finish[3], keypoints_finish[4])\n",
        "    right_eye = (keypoints_finish[6], keypoints_finish[7])\n",
        "    left_ear = (keypoints_finish[9], keypoints_finish[10])\n",
        "    right_ear = (keypoints_finish[12], keypoints_finish[13])\n",
        "    left_shoulder = (keypoints_finish[15], keypoints_finish[16])\n",
        "    right_shoulder = (keypoints_finish[18], keypoints_finish[19])\n",
        "    left_elbow = (keypoints_finish[21], keypoints_finish[22])\n",
        "    right_elbow = (keypoints_finish[24], keypoints_finish[25])\n",
        "    left_wrist = (keypoints_finish[27], keypoints_finish[28])\n",
        "    right_wrist = (keypoints_finish[30], keypoints_finish[31])\n",
        "    left_hip = (keypoints_finish[33], keypoints_finish[34])\n",
        "    right_hip = (keypoints_finish[36], keypoints_finish[37])\n",
        "    left_knee = (keypoints_finish[39], keypoints_finish[40])\n",
        "    right_knee = (keypoints_finish[42], keypoints_finish[43])\n",
        "    left_ankle = (keypoints_finish[45], keypoints_finish[46])\n",
        "    right_ankle = (keypoints_finish[48], keypoints_finish[49])\n",
        "\n",
        "    # 5.1. bend knees\n",
        "    left_knee_ankle = np.array([left_ankle[0] - left_knee[0], left_ankle[1] - left_knee[1]])\n",
        "    left_knee_hip = np.array([left_knee[0] - left_hip[0], left_knee[1] - left_hip[1]])\n",
        "    l_rad_angle = math.acos(np.dot(left_knee_ankle,left_knee_hip) / (np.linalg.norm(left_knee_ankle) * np.linalg.norm(left_knee_hip)))\n",
        "    l_deg_angle = l_rad_angle * (180.0 / math.pi)\n",
        "\n",
        "    right_knee_ankle = np.array([right_ankle[0] - right_knee[0], right_ankle[1] - right_knee[1]])\n",
        "    right_knee_hip = np.array([right_knee[0] - right_hip[0], right_knee[1] - right_hip[1]])\n",
        "    r_rad_angle = math.acos(np.dot(right_knee_ankle,right_knee_hip) / (np.linalg.norm(right_knee_ankle) * np.linalg.norm(right_knee_hip)))\n",
        "    r_deg_angle = r_rad_angle * (180.0 / math.pi)\n",
        "\n",
        "    # compare bending to previous ones\n",
        "    left_bound = prev_l_deg_angle * 0.85\n",
        "    right_bound = prev_r_deg_angle * 0.85\n",
        "    if left_bound < l_deg_angle and right_bound < prev_r_deg_angle:\n",
        "      if 16 in mistakes:\n",
        "        mistakes.remove(16)\n",
        "\n",
        "checkPosition()\n"
      ],
      "metadata": {
        "id": "s652X_ulXlYF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate final output video:"
      ],
      "metadata": {
        "id": "N6cyZiZb3Os4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "long_nr = str('{:03d}'.format(video_nr))\n",
        "video_path = 'input/video' + long_nr + '.mp4'\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "if video.isOpened():\n",
        "  nr_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  fps = video.get(cv2.CAP_PROP_FPS)\n",
        "  width = int(video.get(3))\n",
        "  height = int(video.get(4))\n",
        "  resolution = (width, height)\n",
        "  print(f\"Resolution: {width}x{height}\")\n",
        "\n",
        "  out_video_path = 'output/annotated_video' + long_nr + '.mp4'\n",
        "  out_file = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 7, (width, height))\n",
        "\n",
        "  with open('output/alphapose-results.json') as json_result:\n",
        "      data = json.load(json_result)  #list of dictionaries\n",
        "\n",
        "  counter = 0\n",
        "  thickness = 3\n",
        "  radius_0 = None\n",
        "  radius_1_2 = None\n",
        "  radius_7 = None\n",
        "  while True:\n",
        "    success, frame = video.read()\n",
        "    if not success:\n",
        "      break\n",
        "    keypoints = data[counter]['keypoints']\n",
        "    if counter >= s_start and counter < s_trophy:\n",
        "      #check error codes 0,1,2,3,4\n",
        "      text = []\n",
        "      if (1 in mistakes) or (2 in mistakes):\n",
        "        left_ankle = (keypoints[45], keypoints[46])\n",
        "        right_ankle = (keypoints[48], keypoints[49])\n",
        "        center = (int(min(right_ankle[0], left_ankle[0]) + abs(right_ankle[0] - left_ankle[0])/2),\n",
        "                  int(min(right_ankle[1], left_ankle[1]) + abs(right_ankle[1] - left_ankle[1])/2))\n",
        "        if radius_1_2 == None:\n",
        "          radius_1_2 = (int(abs(right_ankle[0] - left_ankle[0]) * 1.1), int((keypoints[46] - keypoints[40])/2.0))\n",
        "        angle = 0\n",
        "        color = (84, 139, 84) # green in BGR\n",
        "        frame = cv2.ellipse(frame, center, radius_1_2, 0, 0, 360, color, thickness)\n",
        "        # add text\n",
        "        text.append((color, \"Your feet should be slightly wider\"))\n",
        "        text.append((color, \"than shoulder-width apart.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0),\"Your feet have an ideal distance.\"))\n",
        "\n",
        "      if 0 in mistakes:\n",
        "        left_ankle = (int(keypoints[45]), int(keypoints[46]))\n",
        "        if radius_0 == None:\n",
        "          radius_0 = int((keypoints[46] - keypoints[40])/2.0) #(left_ankle.y - left_knee.y) / 2\n",
        "        color = (0, 165, 255) # orange in BGR\n",
        "        frame = cv2.circle(frame, left_ankle, radius_0, color, thickness)\n",
        "        text.append((color,\"Your left foot should be in front\"))\n",
        "        text.append((color,\"of the right one.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"Your left foot is positioned correctly.\"))\n",
        "\n",
        "      if 3 in mistakes:\n",
        "        color = (57, 79, 205)\n",
        "        left_shoulder = (int(keypoints[15]), int(keypoints[16]))\n",
        "        left_elbow = (int(keypoints[21]), int(keypoints[22]))\n",
        "        left_wrist = (int(keypoints[27]), int(keypoints[28]))\n",
        "        frame = cv2.line(frame, left_shoulder, left_elbow, color, thickness)\n",
        "        frame = cv2.line(frame, left_elbow, left_wrist, color, thickness)\n",
        "        text.append((color,\"Let your left arm dangle!\"))\n",
        "      else:\n",
        "        text.append(((0,0,1), \"The position of your left arm\"))\n",
        "        text.append(((0,0,1), \"is perfect.\"))\n",
        "\n",
        "      if 4 in mistakes:\n",
        "        color = (237, 149, 100)\n",
        "        right_shoulder = (int(keypoints[18]), int(keypoints[19]))\n",
        "        right_elbow = (int(keypoints[24]), int(keypoints[25]))\n",
        "        right_wrist = (int(keypoints[30]), int(keypoints[31]))\n",
        "        frame = cv2.line(frame, right_shoulder, right_elbow, color, thickness)\n",
        "        frame = cv2.line(frame, right_elbow, right_wrist, color, thickness)\n",
        "        text.append((color, \"Let your right arm dangle!\"))\n",
        "      else:\n",
        "        text.append(((0,0,2), \"The position of your right arm\"))\n",
        "        text.append(((0,0,2), \"is perfect.\"))\n",
        "\n",
        "      frame_cpy = frame.copy()\n",
        "      vspace = height * 0.03\n",
        "      hspace = width * 0.04\n",
        "      frame = cv2.rectangle(frame, (int(width*2/3), int(height*2/3)), (width, height), (222, 222, 222), cv2.FILLED)\n",
        "      vadd = 0\n",
        "      for i, (c,t) in enumerate(text):\n",
        "        if(i> 0 and c != (0,0,0) and c == text[i-1][0]):\n",
        "          vadd += vspace\n",
        "        else:\n",
        "          vadd += vspace*2\n",
        "          if(c == (0,0,0) or  c == (0,0,1) or c == (0,0,2)):\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, 1)\n",
        "          else:\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, cv2.FILLED)\n",
        "        frame = cv2.putText(frame, t, (int(width*2/3+hspace), int(height*2/3+vadd)), cv2.FONT_HERSHEY_DUPLEX, 0.6, (22, 22, 22), 1)\n",
        "      alpha = 0.7\n",
        "      frame=cv2.addWeighted(frame, alpha, frame_cpy,1-alpha, gamma=0)\n",
        "    #---------------------------------------------------------------------------\n",
        "    elif counter >= s_trophy and counter < s_acc:\n",
        "      #check error codes 5,6,7,8,9,10\n",
        "      text = []\n",
        "      if 5 in mistakes or 6 in mistakes:\n",
        "        color = (237, 149, 100)\n",
        "        right_shoulder = (int(keypoints[18]), int(keypoints[19]))\n",
        "        right_elbow = (int(keypoints[24]), int(keypoints[25]))\n",
        "        right_wrist = (int(keypoints[30]), int(keypoints[31]))\n",
        "        frame = cv2.line(frame, right_shoulder, right_elbow, color, thickness)\n",
        "        frame = cv2.line(frame, right_elbow, right_wrist, color, thickness)\n",
        "        text.append((color, \"Your right arm should be in\"))\n",
        "        text.append((color, \"a 90 degree angle.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"Your right arm has an ideal angle.\"))\n",
        "\n",
        "      if 7 in mistakes:\n",
        "        left_wrist = (int(keypoints[27]), int(keypoints[28]))\n",
        "        if radius_7 == None:\n",
        "          radius_7 = int((keypoints[46] - keypoints[40])/2.0) #(left_ankle.y - left_knee.y) / 2\n",
        "        color = (0, 165, 255) # orange in BGR\n",
        "        frame = cv2.circle(frame, left_wrist, radius_7, color, thickness)\n",
        "        text.append((color, \"Your left arm should be up.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"Your left arm has an ideal position.\"))\n",
        "\n",
        "      if 8 in mistakes:\n",
        "        color = (57, 79, 205)\n",
        "        left_shoulder = (int(keypoints[15]), int(keypoints[16]))\n",
        "        left_elbow = (int(keypoints[21]), int(keypoints[22]))\n",
        "        left_wrist = (int(keypoints[27]), int(keypoints[28]))\n",
        "        frame = cv2.line(frame, left_shoulder, left_elbow, color, thickness)\n",
        "        frame = cv2.line(frame, left_elbow, left_wrist, color, thickness)\n",
        "        text.append((color, \"Stretch your left arm more.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"Your left arm is correctly stretched.\"))\n",
        "\n",
        "      if 9 in mistakes: # or 10 in mistakes:\n",
        "        pt_x = []\n",
        "        pt_y = []\n",
        "        right_shoulder = (keypoints[18], keypoints[19])\n",
        "        right_hip = (keypoints[36], keypoints[37])\n",
        "        right_ankle = (keypoints[48], keypoints[49])\n",
        "        #Bézier curves\n",
        "        for t in np.linspace(0, 1, 100):\n",
        "          x = (1 - t)**2 * right_shoulder[0] + 2 * (1 - t) * t * right_hip[0] + t**2 * right_ankle[0]\n",
        "          y = (1 - t)**2 * right_shoulder[1] + 2 * (1 - t) * t * right_hip[1] + t**2 * right_ankle[1]\n",
        "          pt_x.append(int(x))\n",
        "          pt_y.append(int(y))\n",
        "        pt = np.concatenate((np.array(pt_x).reshape(-1, 1), np.array(pt_y).reshape(-1, 1)), axis=1)\n",
        "        #https://www.geeksforgeeks.org/python-opencv-cv2-polylines-method/\n",
        "        color = (205, 82, 180) # green in BGR\n",
        "        frame = cv2.polylines(frame, [pt.reshape((-1, 1, 2))], False, color, thickness)\n",
        "        text.append((color, \"Tilt your body more backward.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"You have an ideal posture.\"))\n",
        "\n",
        "      frame_cpy = frame.copy()\n",
        "      vspace = height * 0.03\n",
        "      hspace = width * 0.04\n",
        "      frame = cv2.rectangle(frame, (int(width*2/3), int(height*2/3)), (width, height), (222, 222, 222), cv2.FILLED)\n",
        "      vadd = 0\n",
        "      for i, (c,t) in enumerate(text):\n",
        "        if(i> 0 and c != (0,0,0) and c == text[i-1][0]):\n",
        "          vadd += vspace\n",
        "        else:\n",
        "          vadd += vspace*2\n",
        "          if(c == (0,0,0)):\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, 1)\n",
        "          else:\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, cv2.FILLED)\n",
        "        frame = cv2.putText(frame, t, (int(width*2/3+hspace), int(height*2/3+vadd)), cv2.FONT_HERSHEY_DUPLEX, 0.6, (22, 22, 22), 1)\n",
        "      alpha = 0.7\n",
        "      frame=cv2.addWeighted(frame, alpha, frame_cpy,1-alpha, gamma=0)\n",
        "\n",
        "    elif counter >= s_acc and counter < s_contact:\n",
        "      #check error codes 11, 12\n",
        "      text = []\n",
        "      if 11 in mistakes: # or 12 not in mistakes:\n",
        "        pt_x = []\n",
        "        pt_y = []\n",
        "        right_shoulder = (keypoints[18], keypoints[19])\n",
        "        right_hip = (keypoints[36], keypoints[37])\n",
        "        right_ankle = (keypoints[48], keypoints[49])\n",
        "        #Bézier curves\n",
        "        for t in np.linspace(0, 1, 100):\n",
        "          x = (1 - t)**2 * right_shoulder[0] + 2 * (1 - t) * t * right_hip[0] + t**2 * right_ankle[0]\n",
        "          y = (1 - t)**2 * right_shoulder[1] + 2 * (1 - t) * t * right_hip[1] + t**2 * right_ankle[1]\n",
        "          pt_x.append(int(x))\n",
        "          pt_y.append(int(y))\n",
        "        pt = np.concatenate((np.array(pt_x).reshape(-1, 1), np.array(pt_y).reshape(-1, 1)), axis=1)\n",
        "        #https://www.geeksforgeeks.org/python-opencv-cv2-polylines-method/\n",
        "        color = (205, 82, 180) # green in BGR\n",
        "        frame = cv2.polylines(frame, [pt.reshape((-1, 1, 2))], False, color, thickness)\n",
        "        text.append((color, \"Tilt your body more backward.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"You have an ideal posture.\"))\n",
        "\n",
        "      frame_cpy = frame.copy()\n",
        "      vspace = height * 0.03\n",
        "      hspace = width * 0.04\n",
        "      frame = cv2.rectangle(frame, (int(width*2/3), int(height*2/3)), (width, height), (222, 222, 222), cv2.FILLED)\n",
        "      vadd = 0\n",
        "      for i, (c,t) in enumerate(text):\n",
        "        if(i> 0 and c != (0,0,0) and c == text[i-1][0]):\n",
        "          vadd += vspace\n",
        "        else:\n",
        "          vadd += vspace*2\n",
        "          if(c == (0,0,0)):\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, 1)\n",
        "          else:\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, cv2.FILLED)\n",
        "        frame = cv2.putText(frame, t, (int(width*2/3+hspace), int(height*2/3+vadd)), cv2.FONT_HERSHEY_DUPLEX, 0.6, (22, 22, 22), 1)\n",
        "      alpha = 0.7\n",
        "      frame=cv2.addWeighted(frame, alpha, frame_cpy,1-alpha, gamma=0)\n",
        "\n",
        "    elif counter >= s_contact and counter < s_finish:\n",
        "      #check error codes 13, 14, 15\n",
        "      text = []\n",
        "      if 13 in mistakes:\n",
        "        color = (205, 82, 180)\n",
        "        right_shoulder = (int(keypoints[18]), int(keypoints[19]))\n",
        "        right_elbow = (int(keypoints[24]), int(keypoints[25]))\n",
        "        right_wrist = (int(keypoints[30]), int(keypoints[31]))\n",
        "        frame = cv2.line(frame, right_shoulder, right_elbow, color, thickness)\n",
        "        frame = cv2.line(frame, right_elbow, right_wrist, color, thickness)\n",
        "        text.append((color, \"Keep your right arm straight!\"))\n",
        "      else:\n",
        "        text.append(((0,0,1), \"You kept your right arm\"))\n",
        "        text.append(((0,0,1), \"perfectly straight.\"))\n",
        "\n",
        "      if 14 in mistakes:\n",
        "        color = (57, 79, 205)\n",
        "        left_hip = (int(keypoints[33]), int(keypoints[34]))\n",
        "        left_knee = (int(keypoints[39]), int(keypoints[40]))\n",
        "        left_ankle = (int(keypoints[45]), int(keypoints[46]))\n",
        "        frame = cv2.line(frame, left_hip, left_knee, color, thickness)\n",
        "        frame = cv2.line(frame, left_knee, left_ankle, color, thickness)\n",
        "        text.append((color, \"Stretch your left leg more!\"))\n",
        "      else:\n",
        "        text.append(((0,0,2), \"You kept your left leg\"))\n",
        "        text.append(((0,0,2), \"perfectly straight.\"))\n",
        "\n",
        "      if 15 in mistakes:\n",
        "        color = (237, 149, 100)\n",
        "        right_hip = (int(keypoints[36]), int(keypoints[37]))\n",
        "        right_knee = (int(keypoints[42]), int(keypoints[43]))\n",
        "        right_ankle = (int(keypoints[48]), int(keypoints[49]))\n",
        "        frame = cv2.line(frame, right_hip, right_knee, color, thickness)\n",
        "        frame = cv2.line(frame, right_knee, right_ankle, color, thickness)\n",
        "        text.append((color, \"Stretch your right leg more!\"))\n",
        "      else:\n",
        "        text.append(((0,0,3), \"You kept your right leg\"))\n",
        "        text.append(((0,0,3), \"perfectly straight.\"))\n",
        "\n",
        "      frame_cpy = frame.copy()\n",
        "      vspace = height * 0.03\n",
        "      hspace = width * 0.04\n",
        "      frame = cv2.rectangle(frame, (int(width*2/3), int(height*2/3)), (width, height), (222, 222, 222), cv2.FILLED)\n",
        "      vadd = 0\n",
        "      for i, (c,t) in enumerate(text):\n",
        "        if(i> 0 and c != (0,0,0) and c == text[i-1][0]):\n",
        "          vadd += vspace\n",
        "        else:\n",
        "          vadd += vspace*2\n",
        "          if(c == (0,0,0) or c == (0,0,1) or c == (0,0,2) or c == (0,0,3)):\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, 1)\n",
        "          else:\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, cv2.FILLED)\n",
        "        frame = cv2.putText(frame, t, (int(width*2/3+hspace), int(height*2/3+vadd)), cv2.FONT_HERSHEY_DUPLEX, 0.6, (22, 22, 22), 1)\n",
        "      alpha = 0.7\n",
        "      frame=cv2.addWeighted(frame, alpha, frame_cpy,1-alpha, gamma=0)\n",
        "\n",
        "    elif counter >= s_finish and counter < e_finish:\n",
        "      #check error codes 18\n",
        "      text = []\n",
        "      if 16 in mistakes:\n",
        "        left_hip = (int(keypoints[33]), int(keypoints[34]))\n",
        "        left_knee = (int(keypoints[39]), int(keypoints[40]))\n",
        "        left_ankle = (int(keypoints[45]), int(keypoints[46]))\n",
        "        right_hip = (int(keypoints[36]), int(keypoints[37]))\n",
        "        right_knee = (int(keypoints[42]), int(keypoints[43]))\n",
        "        right_ankle = (int(keypoints[48]), int(keypoints[49]))\n",
        "        color = (57, 79, 205)\n",
        "        frame = cv2.line(frame, left_hip, left_knee, color, thickness)\n",
        "        frame = cv2.line(frame, left_knee, left_ankle, color, thickness)\n",
        "        color = (237, 149, 100)\n",
        "        frame = cv2.line(frame, right_hip, right_knee, color, thickness)\n",
        "        frame = cv2.line(frame, right_knee, right_ankle, color, thickness)\n",
        "        text.append((color, \"Bend your knees more after\"))\n",
        "        text.append((color, \"jumping to protect your knees.\"))\n",
        "      else:\n",
        "        text.append(((0,0,0), \"Nice jump!\"))\n",
        "\n",
        "      frame_cpy = frame.copy()\n",
        "      vspace = height * 0.03\n",
        "      hspace = width * 0.04\n",
        "      frame = cv2.rectangle(frame, (int(width*2/3), int(height*2/3)), (width, height), (222, 222, 222), cv2.FILLED)\n",
        "      vadd = 0\n",
        "      for i, (c,t) in enumerate(text):\n",
        "        if(i> 0 and c != (0,0,0) and c == text[i-1][0]):\n",
        "          vadd += vspace\n",
        "        else:\n",
        "          vadd += vspace*2\n",
        "          if(c == (0,0,0)):\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, 1)\n",
        "          else:\n",
        "            frame = cv2.circle(frame, (int(width*2/3+(hspace/2)), int(height*2/3+vadd-vspace/4)), int(vspace/2), c, cv2.FILLED)\n",
        "        frame = cv2.putText(frame, t, (int(width*2/3+hspace), int(height*2/3+vadd)), cv2.FONT_HERSHEY_DUPLEX, 0.6, (22, 22, 22), 1)\n",
        "      alpha = 0.7\n",
        "      frame=cv2.addWeighted(frame, alpha, frame_cpy,1-alpha, gamma=0)\n",
        "\n",
        "    out_file.write(frame)\n",
        "    counter += 1\n",
        "\n",
        "  # Release video capture and writer objects\n",
        "  video.release()\n",
        "  out_file.release()\n",
        "\n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Bf9ZsCCFj1kX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "160e504e-26a8-4735-ab53-582591ed7a24"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "Could not recognize phases\n",
            "No Tennis Racket detected in the given image\n",
            "No Tennis Racket detected in the given image\n",
            "No Tennis Racket detected in the given image\n",
            "No Tennis Racket detected in the given image\n",
            "/content/TennisCoach/AlphaPose\n",
            "68\n",
            "62\n",
            "[5, 6, 7, 8, 9, 10]\n",
            "161.04434933365255\n",
            "175.78202194882817\n",
            "158.6273649010227\n",
            "177.2150737173869\n",
            "170.24682114488922\n",
            "169.85979599223512\n",
            "179.08272883272085\n",
            "160.3521760183981\n",
            "171.86992195473303\n",
            "174.7605019916091\n",
            "174.7605219328832\n",
            "177.015633061071\n",
            "158.00893813338135\n",
            "Resolution: 1280x720\n"
          ]
        }
      ]
    }
  ]
}